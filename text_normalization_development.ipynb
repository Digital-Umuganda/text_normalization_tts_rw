{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "459f5c8c-b664-4899-87e0-454dfb6852b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python'-m pip install git+https://github.com/NVIDIA/NeMo-text-processing.git@main#egg=nemo_text_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a02a2e16-b32b-4d89-94cb-3b44f55a3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynini\n",
    "import nemo_text_processing\n",
    "from pynini.lib import pynutil\n",
    "import os\n",
    "\n",
    "from nemo_text_processing.text_normalization.en.graph_utils import generator_main,GraphFst, NEMO_DIGIT, NEMO_CHAR, NEMO_ALPHA, NEMO_ALNUM, delete_space, NEMO_SIGMA, NEMO_NOT_QUOTE, NEMO_NOT_SPACE, delete_extra_space, NEMO_NON_BREAKING_SPACE\n",
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "\n",
    "from nemo_text_processing.inverse_text_normalization.en.taggers.word import WordFst\n",
    "from nemo_text_processing.inverse_text_normalization.en.taggers.money import MoneyFst\n",
    "\n",
    "from nemo_text_processing.inverse_text_normalization.en.taggers.punctuation import PunctuationFst\n",
    "from nemo_text_processing.inverse_text_normalization.en.verbalizers.money import MoneyFst\n",
    "from nemo_text_processing.inverse_text_normalization.en.verbalizers.time import TimeFst as TimeVerbalizerFst\n",
    "from nemo_text_processing.inverse_text_normalization.en.verbalizers.cardinal import CardinalFst as CardinalVerbalizerFst\n",
    "\n",
    "from nemo_text_processing.inverse_text_normalization.en.verbalizers.word import WordFst as WordVerbalizerFst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf5b69b-9a2a-4c2e-8fc3-7723a112f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynini.lib import pynutil\n",
    "\n",
    "def apply_fst(text, fst):\n",
    "    try:\n",
    "        print(pynini.shortestpath(text @ fst).string())\n",
    "\n",
    "    except pynini.FstOpError:\n",
    "        print(f\"Error: no valid output with given'input: '{text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b11d6f-f1f2-4bbe-af19-4d85b6a5f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transliterations = pynini.string_file(\"kinya_transliterations.tsv\")\n",
    "\n",
    "class WhiteListFst(GraphFst):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"whitelist\", kind=\"classify\")\n",
    "\n",
    "        whitelist = transliterations\n",
    "        graph = pynutil.insert(\"name: \\\"\") + whitelist + pynutil.insert(\"\\\"\")\n",
    "        self.fst = graph.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559203a0-f733-4b66-9c78-63cb9fe4c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardinalFst(GraphFst):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"cardinal\", kind=\"classify\")\n",
    "        # Rest of the grammar here\n",
    "        # ....... \n",
    "        #.........\n",
    "        digit_to_word = {\n",
    "            \"0\": \"zero\",\n",
    "            \"1\": \"imwe\",\n",
    "            \"2\": \"kabiri\",\n",
    "            \"3\": \"gatatu\",\n",
    "            \"4\": \"kane\",\n",
    "            \"5\": \"gatanu\",\n",
    "            \"6\": \"gatandatu\",\n",
    "            \"7\": \"karindwi\",\n",
    "            \"8\": \"munani\",\n",
    "            \"9\": \"icyenda\",\n",
    "        }\n",
    "\n",
    "        # Tens mapping\n",
    "        tens_to_word = {\n",
    "            \"0\": \"\",\n",
    "            \"10\": \"icumi\",\n",
    "            \"20\": \"makumyabiri\",\n",
    "            \"30\": \"mirongo itatu\",\n",
    "            \"40\": \"mirongo ine\",\n",
    "            \"50\": \"mirongo itanu\",\n",
    "            \"60\": \"mirongo itandatu\",\n",
    "            \"70\": \"mirongo irindwi\",\n",
    "            \"80\": \"mirongo inani\",\n",
    "            \"90\": \"mirongo cyenda\",\n",
    "        }\n",
    "    \n",
    "        # Hundreds mapping\n",
    "        hundreds_to_word = {\n",
    "            \"0\":\"\",\n",
    "            \"100\": \"ijana\",\n",
    "            \"200\": \"amagana abiri\",\n",
    "            \"300\": \"magana atatu\",\n",
    "            \"400\": \"magana ane\",\n",
    "            \"500\": \"magana atanu\",\n",
    "            \"600\": \"magana atandatu\",\n",
    "            \"700\": \"magana irindwi\",\n",
    "            \"800\": \"magana inani\",\n",
    "            \"900\": \"magana icyenda\",\n",
    "        }\n",
    "    \n",
    "        # Thousands mapping\n",
    "        thousands_to_word = {\n",
    "            \"0\":\"\",\n",
    "            \"1000\": \"igihumbi\",\n",
    "            \"2000\": \"ibihumbi bibiri\",\n",
    "            \"3000\": \"ibihumbi bitatu\",\n",
    "            \"4000\": \"ibihumbi bine\",\n",
    "            \"5000\": \"ibihumbi bitanu\",\n",
    "            \"6000\": \"ibihumbi bitandatu\",\n",
    "            \"7000\": \"ibihumbi birindwi\",\n",
    "            \"8000\": \"ibihumbi inani\",\n",
    "            \"9000\": \"ibihumbi icyenda\",\n",
    "        }\n",
    "    \n",
    "        # Create FSTs for digits, tens, hundreds, and thousands\n",
    "        digit_fst = pynini.union(*[\n",
    "            pynini.cross(digit, word) for digit, word in digit_to_word.items()\n",
    "        ]).optimize()\n",
    "    \n",
    "        tens_fst = pynini.union(*[\n",
    "            pynini.cross(tens, word) for tens, word in tens_to_word.items()\n",
    "        ]).optimize()\n",
    "    \n",
    "        hundreds_fst = pynini.union(*[\n",
    "            pynini.cross(hundreds, word) for hundreds, word in hundreds_to_word.items()\n",
    "        ]).optimize()\n",
    "    \n",
    "        thousands_fst = pynini.union(*[\n",
    "            pynini.cross(thousands, word) for thousands, word in thousands_to_word.items()\n",
    "        ]).optimize()\n",
    "    \n",
    "        # Combine FSTs for 0-9, 10, 20-90, 100, 1000\n",
    "        combined_fst = digit_fst | tens_fst | hundreds_fst | thousands_fst\n",
    "    \n",
    "        # Combine FSTs for numbers 11-19\n",
    "        eleven_to_nineteen_fst = pynini.union(*[\n",
    "            pynini.cross(num, word) for num, word in {\n",
    "                \"11\": \"icumi na rimwe\",\n",
    "                \"12\": \"icumi na kabiri\",\n",
    "                \"13\": \"icumi na gatatu\",\n",
    "                \"14\": \"icumi na kane\",\n",
    "                \"15\": \"icumi na gatanu\",\n",
    "                \"16\": \"icumi na gatandatu\",\n",
    "                \"17\": \"icumi na karindwi\",\n",
    "                \"18\": \"icumi na munani\",\n",
    "                \"19\": \"icumi na icyenda\",\n",
    "            }.items()\n",
    "        ]).optimize()\n",
    "    \n",
    "        combined_fst |= eleven_to_nineteen_fst\n",
    "    \n",
    "        # Combine FSTs for numbers 21-99 and 101-999\n",
    "        for tens in range(20, 100, 10):\n",
    "            for digit in range(1, 10):\n",
    "                num_str = str(tens + digit)\n",
    "                tens_word = tens_to_word[str(tens)]\n",
    "                digit_word = digit_to_word[str(digit)]\n",
    "                combined_fst |= pynini.cross(num_str, f\"{tens_word} na {digit_word}\")\n",
    "    \n",
    "        for hundreds in range(100, 1000, 100):\n",
    "            for tens in range(0, 100, 10):\n",
    "                for digit in range(0, 10):\n",
    "                    num_str = str(hundreds + tens + digit)\n",
    "                    #print(num_str)\n",
    "                    hundreds_word = hundreds_to_word[str(hundreds)]\n",
    "                    tens_word = tens_to_word[str(tens)]\n",
    "                    digit_word = digit_to_word[str(digit)]\n",
    "                    combined_fst |= pynini.cross(num_str, f\"{hundreds_word} na {tens_word} na {digit_word}\")\n",
    "    \n",
    "        for thousands in range(1000, 10000, 1000):\n",
    "            for hundreds in range(0, 1000, 100):\n",
    "                for tens in range(0, 100, 10):\n",
    "                    for digit in range(0, 10):\n",
    "                        num_str = str(thousands + hundreds + tens + digit)\n",
    "                        thousands_word = thousands_to_word[str(thousands)]\n",
    "                        hundreds_word = hundreds_to_word[str(hundreds)]\n",
    "                        tens_word = tens_to_word[str(tens)]\n",
    "                        digit_word = digit_to_word[str(digit)]\n",
    "                        connector = ''\n",
    "                        combined_fst |= pynini.cross(num_str, f\"{thousands_word} na {hundreds_word} na {tens_word} na {digit_word}\")\n",
    "\n",
    "        graph = combined_fst.optimize()\n",
    "        final_graph = pynutil.insert(\"integer: \\\"\") + graph + pynutil.insert(\" \\\"\")\n",
    "        final_graph = self.add_tokens(final_graph)\n",
    "        self.fst = final_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "945b107c-fc73-43f2-94c5-c08882b2001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elected to use the English cardinal verbalizer, the one below was only included for illustration\n",
    "# class VerbalizeCardinalFst(GraphFst):\n",
    "#     def __init__(self):\n",
    "#         super().__init__(name='cardinal',kind='verbalize')\n",
    "#         graph = (pynutil.delete(\"integer:\")+delete_space+pynutil.delete(\" \\\"\")+pynini.closure(NEMO_CHAR,1)+pynutil.delete(\"\\\"\"))\n",
    "#         delete_tokens = self.delete_tokens(graph)\n",
    "#         self.fst = delete_tokens.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9576d31f-7922-43c0-8687-35d7da161b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFst(GraphFst):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"time\", kind=\"classify\")\n",
    "        \n",
    "        hours = pynini.string_map([\n",
    "            ('1', 'saa saba'),\n",
    "            ('2', 'saa mumani'),\n",
    "            ('3', 'saa cyenda'),\n",
    "            ('4', 'saa kumi'),\n",
    "            ('5', \"saa kumi n'imwe\"),\n",
    "            ('6', \"saa kumi n'ebyiri\"),\n",
    "            ('7', 'saa moya'),\n",
    "            ('8', 'saa mbiri'),\n",
    "            ('9', 'saa tatu'),\n",
    "            ('10', 'saa ine'),\n",
    "            ('11', 'saa tanu'),\n",
    "            ('12', 'saa sita'),\n",
    "\n",
    "        ])\n",
    "        \n",
    "        minutes = pynini.string_map([\n",
    "            ('00', ' '),\n",
    "            ('01', \" n'umunota umwe\") ,\n",
    "            ('02', \" n'iminota ibiri\") ,\n",
    "            ('03', \" n'iminota itatu\") ,\n",
    "            ('04', \" n'iminota ine\") ,\n",
    "            ('05', \" n'iminota itanu\") ,\n",
    "            ('06', \" n'iminota itandatu\") ,\n",
    "            ('07', \" n'iminota irindwi\") ,\n",
    "            ('08', \" n'iminota umunani\") ,\n",
    "            ('09', \" n'iminota icyenda\") ,\n",
    "            ('10', \" n'iminota icumi\") ,\n",
    "            ('11', \" n'iminota cumi n'umwe\") ,\n",
    "            ('12', \" n'iminota cumi n'ibiri\") ,\n",
    "            ('13', \" n'iminota cumi n'itatu\") ,\n",
    "            ('14', \" n'iminota cumi n'ine\") ,\n",
    "            ('15', \" n'iminota cumi n'itanu\") ,\n",
    "            ('16', \" n'iminota cumi n'itandatu\") ,\n",
    "            ('17', \" n'iminota cumi n'irindwi\") ,\n",
    "            ('18', \" n'iminota cumi n'umunani\") ,\n",
    "            ('19', \" n'iminota cumi n'icyenda\") ,\n",
    "            ('20', \" n'iminota makumyabiri\") ,\n",
    "            ('21', \" n'iminota makumyabiri na rimwe\") ,\n",
    "            ('22', \" n'iminota makumyabiri n'ibiri\") ,\n",
    "            ('23', \" n'iminota makumyabiri n'itatu\") ,\n",
    "            ('24', \" n'iminota makumyabiri n'ine\") ,\n",
    "            ('25', \" n'iminota makumyabiri n'itanu\") ,\n",
    "            ('26', \" n'iminota makumyabiri n'itandatu\") ,\n",
    "            ('27', \" n'iminota makumyabiri n'irindwi\") ,\n",
    "            ('28', \" n'iminota makumyabiri n'umunani\") ,\n",
    "            ('29', \" n'iminota makumyabiri n'icyenda\") ,\n",
    "            ('30', \" n'iminota mirongo itatu\") ,\n",
    "            ('31', \" n'iminota mirongo itatu n'umwe\") ,\n",
    "            ('32', \" n'iminota mirongo itatu n'ibiri\") ,\n",
    "            ('33', \" n'iminota mirongo itatu n'itatu\") ,\n",
    "            ('34', \" n'iminota mirongo itatu n'ine\") ,\n",
    "            ('35', \" n'iminota mirongo itatu n'itanu\") ,\n",
    "            ('36', \" n'iminota mirongo itatu n'itandatu\") ,\n",
    "            ('37', \" n'iminota mirongo itatu n'irindwi\") ,\n",
    "            ('38', \" n'iminota mirongo itatu n'umunani\") ,\n",
    "            ('39', \" n'iminota mirongo itatu n'icyenda\") ,\n",
    "            ('40', \" n'iminota mirongo ine\") ,\n",
    "            ('41', \" n'iminota mirongo ine n'umwe\") ,\n",
    "            ('42', \" n'iminota mirongo ine n'ibiri\") ,\n",
    "            ('43', \" n'iminota mirongo ine n'itatu\") ,\n",
    "            ('44', \" n'iminota mirongo ine n'ine\") ,\n",
    "            ('45', \" n'iminota mirongo ine n'itanu\") ,\n",
    "            ('46', \" n'iminota mirongo ine n'itandatu\") ,\n",
    "            ('47', \" n'iminota mirongo ine n'irindwi\") ,\n",
    "            ('48', \" n'iminota mirongo ine n'umunani\") ,\n",
    "            ('49', \" n'iminota mirongo ine n'icyenda\") ,\n",
    "            ('50', \" n'iminota mirongo itanu\") ,\n",
    "            ('51', \" n'iminota mirongo itanu n'umwe\") ,\n",
    "            ('52', \" n'iminota mirongo itanu n'ibiri\") ,\n",
    "            ('53', \" n'iminota mirongo itanu n'itatu\") ,\n",
    "            ('54', \" n'iminota mirongo itanu n'ine\") ,\n",
    "            ('55', \" n'iminota mirongo itanu n'itanu\") ,\n",
    "            ('56', \" n'iminota mirongo itanu n'itandatu\") ,\n",
    "            ('57', \" n'iminota mirongo itanu n'irindwi\") ,\n",
    "            ('58', \" n'iminota mirongo itanu n'umunani\") ,\n",
    "            ('59', \" n'iminota mirongo itanu n'icyenda\") ,\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        final_graph = pynutil.insert(\"hours: \\\"\")+hours+pynutil.insert(\"\\\"\")+pynutil.delete(\":\")+pynutil.insert(\" minutes: \\\"\")+minutes+pynutil.insert(\"\\\"\")\n",
    "\n",
    "        final_graph = self.add_tokens(final_graph)\n",
    "\n",
    "        self.fst = final_graph.optimize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf6b0c09-cfd1-4b4e-9793-c0f4f62fab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time { hours: \"saa sita\" minutes: \" n'iminota mirongo itanu n'ine\" }\n"
     ]
    }
   ],
   "source": [
    "time = TimeFst().fst\n",
    "example = \"12:54\"\n",
    "apply_fst(example, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3766404a-c9c7-4a28-abbc-ef4b8b26ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerbalizeTimeFst(GraphFst):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"time\",kind=\"verbalize\")\n",
    "        hour = (pynutil.delete(\"hours:\")+delete_space+pynutil.delete(\"\\\"\")+pynini.closure(NEMO_CHAR,1,60)+pynutil.delete(\"\\\"\")+delete_space+pynutil.delete(\"minutes:\")+delete_space+pynutil.delete(\"\\\"\")+pynini.closure(NEMO_CHAR,1,60)+pynutil.delete(\"\\\"\"))\n",
    "\n",
    "        graph = hour \n",
    "        delete_tokens = self.delete_tokens(graph)\n",
    "        \n",
    "        self.fst = delete_tokens.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71ebde82-d252-4706-9b59-6bdbeb870472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saa sita n'iminota mirongo itanu n'ine\n"
     ]
    }
   ],
   "source": [
    "verbalize_time = VerbalizeTimeFst().fst\n",
    "apply_fst('time { hours: \"saa sita\" minutes: \" n\\'iminota mirongo itanu n\\'ine\" }', verbalize_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee8c127-e76d-4c8d-bd52-b97957eaa9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyFst(GraphFst):\n",
    "    def __init__(self, cache_dir: str = None, overwrite_cache: bool = False):\n",
    "        super().__init__(name='tokenize_and_classify',kind='classify')\n",
    "        far_file = \"tokenize_and_classify.far\" # None\n",
    "        if cache_dir is not None and cache_dir != \"None\":\n",
    "            os.makedirs(cache_dir, exist_ok=True)\n",
    "            far_file = os.path.join(cache_dir, \"tokenize_and_classify.far\")\n",
    "        if not overwrite_cache and far_file and os.path.exists(far_file):\n",
    "            print(\"FAR file: \",far_file)\n",
    "            self.fst = pynini.Far(far_file, mode=\"r\")[\"TOKENIZE_AND_CLASSIFY\"]\n",
    "        else:\n",
    "            cardinal = CardinalFst()\n",
    "            cardinal_graph = cardinal.fst\n",
    "            time_graph = TimeFst().fst\n",
    "            word_graph = WordFst().fst\n",
    "            punct_graph = PunctuationFst().fst\n",
    "            whitelist_graph = WhiteListFst().fst\n",
    "            classify = (\n",
    "                pynutil.add_weight(time_graph, 1.05)\n",
    "                | pynutil.add_weight(cardinal_graph, 1.1)\n",
    "                | pynutil.add_weight(word_graph, 1.50)\n",
    "                | pynutil.add_weight(whitelist_graph,1.01)\n",
    "            )\n",
    "\n",
    "            punct = pynutil.insert(\"tokens { \") + pynutil.add_weight(punct_graph, weight=1.1) + pynutil.insert(\" }\")\n",
    "            token = pynutil.insert(\"tokens { \") + classify + pynutil.insert(\" }\")\n",
    "            token_plus_punct = (\n",
    "                pynini.closure(punct + pynutil.insert(\" \")) + token+ pynini.closure(pynutil.insert(\" \") + punct)\n",
    "            )\n",
    "    \n",
    "            graph = token_plus_punct + pynini.closure(delete_extra_space + token_plus_punct)\n",
    "            graph = delete_space + graph + delete_space\n",
    "            self.fst = graph.optimize()\n",
    "            if far_file:\n",
    "                print(\"generating grammar\")\n",
    "                generator_main(far_file, {\"TOKENIZE_AND_CLASSIFY\":self.fst})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e85d065e-04af-4c3e-82fb-efa70e590d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Created tokenize_and_classify.far\n",
      "INFO:NeMo-text-processing:Created tokenize_and_classify.far\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating grammar\n"
     ]
    }
   ],
   "source": [
    "classify = ClassifyFst().fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12b6f012-c241-4200-9b92-0efa823d03c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens { name: \"byatangiye\" } tokens { time { hours: \"saa ine\" minutes: \" n'iminota mirongo itatu\" } } tokens { name: \"haje\" } tokens { name: \"abantu\" } tokens { cardinal { integer: \"ibihumbi bibiri \" } } tokens { name: \"niwo\" } tokens { name: \"mubare\" } tokens { name: \"wabakora\" } tokens { name: \"muri\" } tokens { time { hours: \"saa sita\" minutes: \" n'iminota ibiri\" } }\n"
     ]
    }
   ],
   "source": [
    "apply_fst(\"byatangiye 10:30 haje abantu 2000 niwo mubare wabakora muri 12:02\",classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ec71598-4262-4c6c-b0be-4ff6683b123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerbalizeTimeFst(GraphFst):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"time\",kind=\"verbalize\")\n",
    "        hour = (pynutil.delete(\"hours:\")+delete_space+pynutil.delete(\"\\\"\")+pynini.closure(NEMO_CHAR,1,60)+pynutil.delete(\"\\\"\")+delete_space+pynutil.delete(\"minutes:\")+delete_space+pynutil.delete(\"\\\"\")+pynini.closure(NEMO_CHAR,1,60)+pynutil.delete(\"\\\"\"))\n",
    "\n",
    "        graph = hour \n",
    "        delete_tokens = self.delete_tokens(graph)\n",
    "        self.fst = delete_tokens.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce5b27a8-cf75-46a7-8be9-d38641090c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VerbalizeFst(GraphFst):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"verbalize\", kind=\"verbalize\")\n",
    "        cardinal = CardinalVerbalizerFst()#VerbalizeCardinalFst()\n",
    "        cardinal_graph = cardinal.fst\n",
    "        time = VerbalizeTimeFst().fst\n",
    "\n",
    "        graph = (\n",
    "            cardinal_graph\n",
    "           | time\n",
    "        )\n",
    "        self.fst = graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78483280-9d41-41ce-9824-c285909158ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VerbalizeFinalFst(GraphFst):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"verbalize_final\", kind=\"verbalize\")\n",
    "        verbalize = VerbalizeFst().fst\n",
    "        word = WordVerbalizerFst().fst\n",
    "        \n",
    "        types = verbalize | word\n",
    "        graph = (\n",
    "            pynutil.delete(\"tokens\")\n",
    "            + delete_space\n",
    "            + pynutil.delete(\"{\")\n",
    "            + delete_space\n",
    "            + types\n",
    "            + delete_space\n",
    "            + pynutil.delete(\"}\")\n",
    "        )\n",
    "        graph = delete_space + pynini.closure(graph + delete_extra_space) + graph + delete_space\n",
    "        graph = delete_space + pynini.closure(graph + delete_extra_space) + graph + delete_space\n",
    "\n",
    "        self.fst = graph\n",
    "        far_file = \"verbalize.far\"\n",
    "        generator_main(far_file, {\"ALL\":self.fst,'REDUP': pynini.accep(\"REDUP\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9840ef8-f4fe-4438-93aa-ebf9eb3aaf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Created verbalize.far\n",
      "INFO:NeMo-text-processing:Created verbalize.far\n"
     ]
    }
   ],
   "source": [
    "final_verbalize = VerbalizeFinalFst().fst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75d185eb-eb3a-4bcf-9595-0f8a2f5eb50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byatangiye saa inen'iminota mirongo itatu haje abantu ibihumbi bibiri  niwo mubare wabakora muri saa sitan'iminota ibiri\n"
     ]
    }
   ],
   "source": [
    "apply_fst('tokens { name: \"byatangiye\" } tokens { time { hours: \"saa ine\" minutes: \"n\\'iminota mirongo itatu\" } } tokens { name: \"haje\" } tokens { name: \"abantu\" } tokens { cardinal { integer: \"ibihumbi bibiri \" } } tokens { name: \"niwo\" } tokens { name: \"mubare\" } tokens { name: \"wabakora\" } tokens { name: \"muri\" } tokens { time { hours: \"saa sita\" minutes: \"n\\'iminota ibiri\" } } ',final_verbalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65e3d183-8052-40d0-8a56-a9eefb79216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbc4ab-c726-481e-808b-c76fa5eef038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
